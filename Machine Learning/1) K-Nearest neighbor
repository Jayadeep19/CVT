# KNN classifier is implemented using the libraries (Numpy, Pandas,SciKit Learn, Matplotlib)


import numpy as np
import matplotlib.pyplot as plt
import pandas as pd


# Load the dataset
url = "https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data"

# First assign the names of the columns for the dataset
names = ['sepal-length','sepal-width','petal-length','petal-width','class']

#Now read the dataset to pandas dataframe
dataset = pd.read_csv(url, names = names)

dataset.head()

X = dataset.iloc[:,:-1].values
Y = dataset.iloc[:,4].values

# print(dataset.iloc[:,4].values)
#print (X)
#print (Y)

# Split the data into training and testing data sets using train_test_split function

from sklearn.model_selection import train_test_split
X_train, X_test, Y_train, Y_test = train_test_split(X, Y , test_size = 0.20)

# print(X_train.shape,'\n' , X_test.shape)


# Normalising the features is a very good practice for efficient and accurate predictions.
# Use the function StandardScaler from the preprocessing toolbox

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
scaler.fit(X_train)   # To find the mean and std that can be used later

#print(scaler.mean_)

X_train = scaler.transform(X_train)   # Performs actual standardization
X_test = scaler.transform(X_test)

#print(X_train)


# Now train the model on the transformed training set

from sklearn.neighbors import KNeighborsClassifier
classifier = KNeighborsClassifier(n_neighbors = 5)
classifier.fit(X_train, Y_train)


# Now the predicitons can be made on the test data to test the performance of our KNN

Y_predicted = classifier.predict(X_test)

# print(Y_test, '\n', Y_predicted)


# Evaluating the algorithm by using the confusion matrix and classification report

from sklearn.metrics import classification_report, confusion_matrix
print(confusion_matrix(Y_test, Y_predicted))     #use the corret ground truth values for the confusion matrices
print(classification_report(Y_test, Y_predicted))


# We can find the Values of K which gives the best results by running a for loop between say 1 and 40
#Initilize a list and append the error values later to the empyt list
error = []

for k in range(1,40):
    clsfy = KNeighborsClassifier(n_neighbors = k)
    clsfy.fit(X_train, Y_train)
    pred_k = clsfy.predict(X_test)
    error.append(np.mean(pred_k != Y_test))

    
#Now we can plot the values of K vs the error at different K-values

plt.figure(figsize = (12,6))
plt.plot(range(1,40), error, 
         color = 'red', linestyle = 'dotted', marker = 'o',
         markerfacecolor = 'blue', markersize = 10)
plt.title('Error rate K value')
plt.xlabel('K-Values')
plt.ylabel('Mean Error')


'''
Summary
KNN is the simplest classifier that calculates the distance between the new point and the already existing training data points.
K can be any integer value that defines the number of nearest points that the classifier looks for and makes the prediction for the target class of the new data point

 Steps to implement the K- Nearest neighbor classifier:
1) Loading the libraries 
    We need - Numpy, Matplotlib, Pandas, Scikit Learn
2) Importing the required dataset
3)Reading the dataset to local Pandas dataframe
4)Use the data to create variables that contains the information of the class and the true ground 
    values(classes)
5)T Now, that the data is organised, split the data into training and test datasets with the desired size
    for the test and training sets
6)Normalisation, this is an Important step to make all the features range equal i.e every feature contributes 
    proportionally to the final prediction
7)Train the classifier with training data
8)Test the predicitons using the test data.
9) Finally, evaluate the algorithm using the confusion matrix and classification report
'''

