'''
In the following code, the previously created perceptron is trained and the weights of the inpiuts are updated.
The perceptron in the first epoch, calculates the weighted sum of the inputs and sends to the sigmoid activation function.
The sigmoid bounds the range of the outputs to [0,1] and outputs the final values.

These outputs are then compared to the true outputs and the error is calculated.
Now, the error is multipied with the gradient of the sigmoid (more sensitive to the error value) and 
finally dot product between result and Inputs gives the final output values.
This completes one epoch and when the number of epochs reaches infinity the final outputs reach the true output values.
'''

import numpy as np

def sigmoid(x):
    return 1/(1 + np.exp(-x))

def sigmoid_der(x):
    return x * (1-x)

training_inputs = np.array([[0,0,1],
                           [1,1,1],
                           [1,0,1],
                            [0,1,1]]) 

training_outputs = np.array([[0,1,1,0]]).T
print(training_inputs.shape)

np.random.seed(1)     #to get the same random numbers

synaptic_weights = 2 * np.random.random((3,1)) -1          #weights for the 3 inputs, create a 3*1 array


print('Random starting synaptic weights :')
print(synaptic_weights)

for iteration in range(20000):

    input_layer = training_inputs
    output_layer = sigmoid(np.dot(input_layer, synaptic_weights))

    #updation of weights following the formulae: update = error * input * gradient of the sigmoid

    error = training_outputs - output_layer
    adjustments = error * sigmoid_der(output_layer)
    synaptic_weights += np.dot(input_layer.T, adjustments)    

print('synaptic weights after training :\n {}'.format(synaptic_weights))

print('Outputs after training :\n {}'.format(output_layer)



